How to log all queries:
  set global log_output = 'FILE';
  set global general_log_file = '/tmp/mysql.log';
  set global general_log = "ON";


Run the MySQL test suite:
MTR_MAX_TEST_FAIL=1000 ./mysql-test-run.pl --force --suite=engines/funcs --mysqld=--default-storage-engine=upscaledb --mysqld=--plugin_load=ha_upscaledb.so --mysqld=--default-tmp-storage-engine=upscaledb engines/funcs.crash_manyindexes_number
    - engines/func
    - engines/iuds




x Complex keys made up of multiple columns have different sort order than
    in innodb (i.e. tpcc/customer table)
    PRIMARY KEY (`c_w_id`,`c_d_id`,`c_id`)
    -> verify this once more
    -> yes, but upscaledb's sort order is clearly defined and seems to be
        more correct

x Is the MaxKeyCache properly initialized when the database is initialized?
    -> yes

o There's a race condition when using the MaxKeyCache:
    - in compare_and_update(), which requires a mutex
    - between querying the cache and actually performing the database operation
    - there are more issues; the cache is not updated when a key is deleted
    x would it make sense to move this to upscaledb core? - no, not required
    x use scoped_ptr for the MaxKeyCache - not reqd
    o remove the MaxKeyCache; the upscaledb core already has a similar
        optimization in btree_insert.cc: it uses the hinter to decide
        if append/prepend is possible!
        o make UPS_HINT_APPEND, UPS_HINT_PREPEND internal
        o store the "historgram" in the database statistics
        o initialize on demand
        o update it when inserting, deleting
        o clean up logic in btree_insert.cc
        o txn_insert: if UPS_HINT_APPEND/-PREPEND is set then do not
            look up the btree! 

        o insert optimization: if the key is appended or prepended then
            skip the btree lookup (via the UPS_HINT_* flag)
        o erase optimization: if the deleted item is in the histogram
            then delete it (the key must then be re-initialized whenever it's
            accessed again)
        o txn lookup optimization: if the key is outside of the histogram
            then fail immediately (only if the histogram was initialized)

        o btree insert optimization: if the last 5 inserts were at the same
            page, then check:
                hint-append: if page has no right sibling and is not full:
                    append directly to page
                hint-prepend: if page has no left sibling and is not full:
                    prepend directly to page
                if the page is not full:
                    insert directly into the page
        o do the same for lookups
        o do the same for deletes

o It seems that MySQL's handler class already has functions to pack and unpack
    keys, i.e. see calculate_key_len(). Can we switch to these functions,
    instead of using our own?
    -> no, leave as is; switching has no benefits
    o implement index_read_last instead of index_read_map_last
    o implement index_read instead of index_read_map

o Implement BEGIN and COMMIT, otherwise sysbench fails when running it with
    parallel connections (see https://github.com/akopytov/sysbench/issues/65)
    -> A writing conflict needs to block; this needs changes in upscaledb
    o see mysql-tests/suite/engines/README (item #3) on how to test this
    o What if transactions span multiple tables?? Then they have to be
        shared between all upscaledb environments!
        or - better solution (although with more effort): only use one
        upscaledb environment per database!
        o need a new way of mapping tables/indices to database names
        o rewrite the existing code; all tests need to work!

    o identify the callback functions in the handler that have to be
        implemented
        -> handler::external_lock()
        -> handler::start_stmt()
        -> handler::end_stmt() (a.k.a reset())
    o upscaledb needs to block in case of a conflict (new behaviour - add
        a flag when creating the Transaction!)
        o add flag to wrapper libraries
        o OR: new API to discover which Transaction is blocking, and a new
            API to wait for it (incl. timeout)
        -> verify this! InnoDB blocks, but has a timeout. Either we
          have to pass the timeout to the Transaction, or the Transaction is
          blocked in MySQL (not InnoDB)!

o Implement index condition pushdown
    -> pass a function pointer to the upscaledb core (register it as a
        callback), then use a new "UQI scan" method to fast-forward
        the cursor
    o re-run wordpress benchmarks

o implement UpscaledbHandler::records_in_range()
o implement UpscaledbHandler::records() (does this make things faster??)
o implement UpscaledbHandler::extra()
o implement UpscaledbHandler::info()
    innobase_hton->show_status = innobase_show_status;
    -> print info of each environment and of each server, including all
        configuration parameters (page size, cache size etc)
o implement handler::check()

o TPC-C does not yet work; the loader seems to be fine, but tpcc_run fails
    LD_LIBRARY_PATH=/usr/local/mysql/lib time ./tpcc_load -h localhost -d tpcc1000i -u root -p "" -w  1
    LD_LIBRARY_PATH=/usr/local/mysql/lib time ./tpcc_start -h127.0.0.1 -P3306 -dtpcc1000u -uroot -w1 -c32 -r10 -l10800

    x load upscaledb and innodb with 1 warehouse; compare both databases
        -> they're ok (except that compound keys are sorted differently)
    o then start running the tests; compare queries (payment and ordstat
        seem to fail)
        x can we fill each database separately? - no
        x enable query logging
        x write a perl script which executes the queries simultaneously on
            both databases and compares the results
        x after fill: SELECT count(c_id) FROM customer WHERE c_w_id = 1 AND c_d_id = 7 AND c_last = 'ABLEESEPRI';
            -> result is 79, must be 15!
            -> theory: only the first column (c_w_id) is compared, but not the
                second one (in UpscaledbHandler::read_next_same)
        o still get errors during "ramp-up time" (and in sysbench)
            -> this is caused by concurrency issues. it is NOT a load issue -
                idb and ups create identical databases. but when running
                tpcc_start with 1 connection then everything is correct.
            -> otoh, loading is always with 1 connection; we therefore can't
                proof that loading is correct
            -> sysbench also fails when using it with multiple threads.
            x is this only related to the AUTO_INCREMENT field? hack sysbench
                and insert a "real" ID instead of a null value. See if the
                problem goes away
                -> there's an option "oltp_auto_inc" == "off"
                -> no, the problem persists
        o and when running: 1030, HY000, Got error 1 from storage engine
            -> might be related
    ==> requires implementation of BEGIN/COMMIT

o properly close the environments when the server is shut down

. allow in-memory environments (comment option: "in_memory=true|false")
    currently would not work because ::create() closes the Environment,
    ::open() opens it. In-memory environments have to remain open!
    -> when creating, store the UpscaledbData globally; do not close the
        Environment again (and therefore do not open it again in ::open())

. per-index options via a "CREATE TABLE" comment
    o integer compression (works with and without duplicates)

. how can we improve performance of duplicate keys?
    cheap solution: split duplicate table in blocks, only merge blocks when
        they're empty
    better solution (more effort): blobs form a linked list
        pro: index will become smaller and therefore faster
        pro: less code
        con: how to consolidate them then?
    -> see upscaledb's TODO file

. need a code generator for the remote interface that shows how to
    de-serialize the database records

. identify the 3 most commonly used MySQL operations in Wordpress; use
    the API instead and see if it improves performance
    -> can we move this to a wordpress plugin??

. engines/iuds.strings_charsets_update_delete
    -- select * from t8 where a like 'uu%';
    -- index_read_map locates the second key via approx. matching. The
        first key is not a match because it starts with a 'U' instead
        of a 'u'.
    -- however, the mysql collation is case-insensitive and therefore
        includes 'U'!
    -- have to compare using the collation of the current field, if there
        is one; if necessary, move the cursor "to the left"

o public release ---------------------------------------------------------

. CREATE TABLE (id integer primary key) -> does not have to store any
    records! (but it does)

o secondary keys should not require a lookup of the primary key;
    directly store the blob id of the record
    -> look at BDB, they have a similar API

o online DDL:
    http://dev.mysql.com/doc/refman/5.6/en/innodb-create-index-overview.html

o when updating: re-use the cursor by temporarily attaching it to the
    current transaction (is that possible?)
    (is this still relevant when we use global BEGIN/COMMIT transactions?)

o support encryption
    m_create_info->used_fields & HA_CREATE_USED_PASSWORD (handler.h)
    https://dev.mysql.com/doc/refman/5.7/en/innodb-tablespace-encryption.html
    using a cheap solution or an expensive one?

. improve auto-inc performance; the current code always calls index_last() to
    get the last known value. Use recno-databases instead!
    but remember that it's possible to use UPDATE to change the key, as long as
    the key was not yet used.
    -> or can we cache index_last() instead?

